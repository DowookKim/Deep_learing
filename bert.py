# -*- coding: utf-8 -*-
"""bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ku3GTwKjPObcLv0ZMHD29LBHrBzSFQvh
"""

!pip install transformers

import pandas as pd
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

result = tokenizer.tokenize("Here is the sentence I want embeddings for")
print(result)

print(tokenizer.vocab['here'])

print(tokenizer.vocab['embeddings'])

print(tokenizer.vocab['em'])
print(tokenizer.vocab['##bed'])
print(tokenizer.vocab['##ding'])
print(tokenizer.vocab['##s'])

# BERT의 단어 집합을 vocabulary.txt에 저장
with open('vocabulary.txt', 'w') as f:
  for token in tokenizer.vocab.keys():
    f.write(token + '\n')

df = pd.read_fwf('vocabulary.txt', header=None)
df

print('단어 집합의 크기 :',len(df))
df.loc[4667].values[0]

"""# Bert 모델 불러와서 써보기

"""

from transformers import TFBertForMaskedLM
from transformers import AutoTokenizer
#BERT를 마스크드 언어 모델 형태로 로드하기 위한 import

from transformers import AutoTokenizer, AutoModelForMaskedLM
model = AutoModelForMaskedLM.from_pretrained("bert-large-uncased")
tokenizer = AutoTokenizer.from_pretrained("bert-large-uncased")
'''Google이 공개한
BERT-large (24-layer, 1024 hidden)
Wikipedia + BooksCorpus로
MLM (+ NSP) 프리트레이닝만 수행한 가중치
버트 모델'''
#AutoTokenizer.from_pretrained('모델 이름')을 넣으면 해당 모델이 학습되었을 당시에 사용되었던 토크나이저를 로드

inputs = tokenizer(
    "Soccer is a really fun [MASK].",
    return_tensors="pt"
)
#마스크드 언어 모델은 [MASK]의 위치에 해당하는 단어를 예측

print(inputs['input_ids'])

print(inputs['token_type_ids'])
#크나이저로 변환된 결과에서 token_type_ids를 통해서 문장을 구분하는 세그먼트 인코딩 결과를 확인
#현재의 입력은 문장이 두 개가 아니라 한 개이므로 여기서는 문장 길이만큼의 0 시퀀스

print(inputs['attention_mask'])
#현재의 입력에서는 패딩이 없으므로 여기서는 문장 길이만큼의 1 시퀀스를

from transformers import FillMaskPipeline
pip = FillMaskPipeline(model=model, tokenizer=tokenizer)

pip('Soccer is a really fun [MASK].')

pip('The Avengers is a really fun [MASK].')

pip('I went to [MASK] this morning.')

"""# 한국어 BERT의 마스크드 언어 모델"""

from transformers import TFBertForMaskedLM
from transformers import AutoTokenizer

model = TFBertForMaskedLM.from_pretrained('klue/bert-base', from_pt=True)
tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")

inputs = tokenizer('축구는 정말 재미있는 [MASK]다.', return_tensors='tf')

print(inputs['input_ids'])

print(inputs['token_type_ids'])

print(inputs['attention_mask'])

from transformers import FillMaskPipeline
pip = FillMaskPipeline(model=model, tokenizer=tokenizer)

pip('축구는 정말 재미있는 [MASK]다.')

pip('어벤져스는 정말 재미있는 [MASK]다.')

pip('나는 오늘 아침에 [MASK]에 출근을 했다.')

"""# 구글 BERT의 다음 문장 예측"""

import tensorflow as tf
from transformers import TFBertForNextSentencePrediction
from transformers import AutoTokenizer
#TFBertForNextSentencePrediction.from_pretrained('BERT 모델 이름')을 넣으면 두 개의 문장이 이어지는 문장 관계인지 여부를 판단하는 BERT 구조를 로드

from transformers import TFBertForNextSentencePrediction, AutoTokenizer
#TFBertForNextSentencePrediction.from_pretrained('BERT 모델 이름')을 넣으면 두 개의 문장이 이어지는 문장 관계인지 여부를 판단하는 BERT 구조를 로드
model = TFBertForNextSentencePrediction.from_pretrained(
    "bert-base-uncased",
    from_pt=True,              # PyTorch → TF 변환 로드
    use_safetensors=False      # safetensors 비활성화
)
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
#AutoTokenizer.from_pretrained('모델 이름')을 넣으면 해당 모델이 학습되었을 당시에 사용되었던 토크나이저를 로드

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
next_sentence = "pizza is eaten with the use of a knife and fork. In casual settings, however, it is cut into wedges to be eaten while held in the hand."

encoding = tokenizer(prompt, next_sentence, return_tensors='tf')

print(encoding['input_ids'])

print(tokenizer.cls_token, ':', tokenizer.cls_token_id)
print(tokenizer.sep_token, ':' , tokenizer.sep_token_id)

print(tokenizer.decode(encoding['input_ids'][0]))

print(encoding['token_type_ids'])
'''
0이 연속적으로 등장하다가 어느 순간부터 1이 연속적으로 등장하는데, 이는 [CLS] 토큰의 위치부터 첫번째 문장이 끝나고나서 등장한 [SEP] 토큰까지의 위치에는 0이 등장하고, 다음 두번째 문장부터는 1이 등장하는 것입니다. token_type_ids에서는 0과 1로 두 개의 문장을 구분하고 있습니다.
'''

logits = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])[0]
softmax = tf.keras.layers.Softmax()
probs = softmax(logits)
print(probs)

print('최종 예측 레이블 :', tf.math.argmax(probs, axis=-1).numpy())
'''
최종 예측 레이블은 0입니다. 이는 BERT가 다음 문장 예측을 학습했을 당시에 실질적으로 이어지는 두 개의 문장의 레이블은 0. 이어지지 않는 두 개의 문장의 경우에는 레이블을 1로 두고서 이진 분류로 학습을 하였기 때문입니다
'''

# 상관없는 두 개의 문장
prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
next_sentence = "The sky is blue due to the shorter wavelength of blue light."
encoding = tokenizer(prompt, next_sentence, return_tensors='tf')

logits = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])[0]

softmax = tf.keras.layers.Softmax()
probs = softmax(logits)
print('최종 예측 레이블 :', tf.math.argmax(probs, axis=-1).numpy())

"""# 한국어 BERT의 다음 문장 예측"""

import tensorflow as tf
from transformers import TFBertForNextSentencePrediction
from transformers import AutoTokenizer
#TFBertForNextSentencePrediction.from_pretrained('BERT 모델 이름')을 넣으면 두 개의 문장이 이어지는 문장 관계인지 여부를 판단하는 BERT 구조를 로드

from transformers import TFBertForNextSentencePrediction, AutoTokenizer
#TFBertForNextSentencePrediction.from_pretrained('BERT 모델 이름')을 넣으면 두 개의 문장이 이어지는 문장 관계인지 여부를 판단하는 BERT 구조를 로드
model = TFBertForNextSentencePrediction.from_pretrained(
    'klue/bert-base',
    from_pt=True,              # PyTorch → TF 변환 로드
    use_safetensors=False      # safetensors 비활성화
)
tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')
#AutoTokenizer.from_pretrained('모델 이름')을 넣으면 해당 모델이 학습되었을 당시에 사용되었던 토크나이저를 로드

# 이어지는 두 개의 문장
prompt = "2002년 월드컵 축구대회는 일본과 공동으로 개최되었던 세계적인 큰 잔치입니다."
next_sentence = "여행을 가보니 한국의 2002년 월드컵 축구대회의 준비는 완벽했습니다."
encoding = tokenizer(prompt, next_sentence, return_tensors='tf')

logits = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])[0]

softmax = tf.keras.layers.Softmax()
probs = softmax(logits)
print('최종 예측 레이블 :', tf.math.argmax(probs, axis=-1).numpy())

# 상관없는 두 개의 문장
prompt = "2002년 월드컵 축구대회는 일본과 공동으로 개최되었던 세계적인 큰 잔치입니다."
next_sentence = "극장가서 로맨스 영화를 보고싶어요"
encoding = tokenizer(prompt, next_sentence, return_tensors='tf')

logits = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])[0]

softmax = tf.keras.layers.Softmax()
probs = softmax(logits)
print('최종 예측 레이블 :', tf.math.argmax(probs, axis=-1).numpy())

"""# BERT의 문장 임베딩(SBERT)을 이용한 한국어 챗봇"""

!pip install sentence_transformers

import numpy as np
import pandas as pd
from numpy import dot
from numpy.linalg import norm
import urllib.request
from sentence_transformers import SentenceTransformer

urllib.request.urlretrieve("https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv", filename="ChatBotData.csv")
train_data = pd.read_csv('ChatBotData.csv')
train_data.head()

model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')
#문장 임베딩을 얻기 위해서 사전 훈련된 BERT를 로드
'''
모델의 이름은 'xlm-r-100langs-bert-base-nli-stsb-mean-tokens'인데 이름이 의미하는 바는 100가지 언어를 지원(한국어 포함)하는 다국어 BERT BASE 모델로 SNLI 데이터를 학습 후 STS-B 데이터로 학습되었으며, 문장 표현을 얻기 위해서는 평균 풀링(mean-tokens)을 사용했다는 의미
다시 말해서 NLI 데이터를 학습 후에 STS 데이터로 추가 파인 튜닝한 모델
'''

train_data['embedding'] = train_data.apply(lambda row: model.encode(row.Q), axis = 1)

def cos_sim(A, B):
  return dot(A, B)/(norm(A)*norm(B))

def return_answer(question):
    embedding = model.encode(question)
    train_data['score'] = train_data.apply(lambda x: cos_sim(x['embedding'], embedding), axis=1)
    return train_data.loc[train_data['score'].idxmax()]['A']

return_answer('결혼하고싶어')

return_answer('나랑 커피먹을래?')

return_answer('퇴근하고싶어')