KoGPT-2로 문장 생성하기
import numpy as np
import random
import tensorflow as tf
from transformers import AutoTokenizer
from transformers import TFGPT2LMHeadModel

model = TFGPT2LMHeadModel.from_pretrained(
    'skt/kogpt2-base-v2',
    from_pt=True,              # PyTorch → TF 변환 로드
    use_safetensors=False      # safetensors 비활성화
)
tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')
'''
TFGPT2LMHeadModel.from_pretrained('GPT 모델 이름')을 넣으면 두 개의 문장이 이어지는 문장 관계인지
여부를 판단하는 GPT 구조를 로드합니다.
AutoTokenizer.from_pretrained('모델 이름')을 넣으면 해당 모델이 학습되었을 당시에
사용되었던 토크나이저를 로드
'''

sent = '근육이 커지기 위해서는'
#GPT가 생성할 문장의 방향성을 알려주기 위해서 시작 문자열을 정해줌

input_ids = tokenizer.encode(sent)
input_ids = tf.convert_to_tensor([input_ids])
print(input_ids)

#GPT의 입력으로는 정수 인코딩 된 결과가 입력되어야 하므로 tokenizer.encode()를 통해서
#'근육이 커지기 위해서는'이라는 문자열을 정수 시퀀스로 변환
output = model.generate(input_ids,
                        max_length=128,
                        repetition_penalty=2.0,
                        use_cache=True)
output_ids = output.numpy().tolist()[0]
print(output_ids)

'''
33245 10114 12748 11357라는 5개의 정수 시퀀스를 얻습니다.
해당 정수 시퀀스를 GPT의 입력으로 사용하여 GPT가 이어서 문장을 생성하도록 해봅시다.
주어진 문장으로부터 이어서 문장을 생성하도록 하는 것은 model.generate()를 사용합니다
'''

'''
기존의 33245 10114 12748 11357 라는 5개의 정수
시퀀스 뒤에도 여러 정수들이 추가로 생성된 것을 볼 수 있습니다.
정수들이 단순히 나열된 것만으로는 GPT가 실제로 어떤 문장을 생성했는지 알기 어려우니 해당 정수 시퀀스를
한국어로 변환해봅시다.
이 과정은 tokenizer.decode()를 사용하여 가능
'''

tokenizer.decode(output_ids)

